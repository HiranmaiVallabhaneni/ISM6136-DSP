{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression, SVM and DTree Classifer along with Neural Network\n",
    "\n",
    "In this we will demonstarte how to use Logistic Regression in scikit-learn to perfrom logistic regression\n",
    "\n",
    "In this tutorial we will demonstrate how to use the SVM class in scikit-learn to perform logistic regression on a dataset.\n",
    "\n",
    "In this tutorial we will demonstrate how to use the `DecisionTreeClassifer` class in `scikit-learn` and we use boosting techniques to perform classifications predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup\n",
    "Import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from __future__ import print_function\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load data\n",
    "Load data (it's already cleaned and preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following snippet of code to debug problems with finding the .csv file path\n",
    "# This snippet of code will exit the program and print the current working directory.\n",
    "#import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"thoracic_train_X_risk.csv\")\n",
    "X_test = pd.read_csv(\"thoracic_test_X_risk.csv\")\n",
    "y_train = pd.read_csv(\"thoracic_train_y_risk.csv\")\n",
    "y_test = pd.read_csv(\"thoracic_test_y_risk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a dataframe to load the model performance metrics into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic regression using Randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 932, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "910 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "290 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.86018648 0.84801865        nan 0.84498834 0.86018648 0.83589744\n",
      " 0.83589744 0.84498834 0.84498834        nan        nan        nan\n",
      "        nan 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.86018648        nan 0.86018648 0.85109557        nan 0.84806527\n",
      " 0.84498834 0.85109557 0.86018648 0.86018648        nan 0.86018648\n",
      "        nan 0.83892774 0.83589744 0.83589744 0.84806527        nan\n",
      " 0.84801865 0.86018648 0.86018648 0.86018648        nan 0.84801865\n",
      "        nan 0.86018648        nan        nan        nan 0.86018648\n",
      "        nan        nan        nan 0.86018648 0.86018648        nan\n",
      "        nan 0.83897436        nan 0.84806527        nan        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.83897436 0.84498834 0.86018648\n",
      "        nan        nan 0.84498834 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.86018648        nan 0.85109557        nan\n",
      "        nan        nan 0.84806527 0.84498834 0.83892774 0.84498834\n",
      " 0.84498834 0.86018648 0.84498834        nan        nan        nan\n",
      "        nan        nan 0.83589744 0.84498834        nan 0.83589744\n",
      "        nan 0.83589744 0.86018648 0.84498834 0.84498834 0.86018648\n",
      " 0.83892774        nan        nan 0.84498834        nan        nan\n",
      "        nan 0.86018648 0.85109557 0.84806527 0.86018648        nan\n",
      "        nan        nan 0.84498834 0.84498834        nan        nan\n",
      " 0.83892774 0.84498834        nan 0.84801865 0.86018648 0.86018648\n",
      " 0.86018648 0.84498834        nan 0.84498834        nan        nan\n",
      " 0.84498834        nan        nan 0.84498834 0.83892774 0.84498834\n",
      " 0.86018648        nan        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.86018648 0.83589744        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.83589744 0.85109557        nan 0.84498834 0.85109557\n",
      "        nan        nan 0.86018648        nan 0.84498834 0.84498834\n",
      " 0.83892774 0.84498834 0.83897436        nan        nan 0.84498834\n",
      " 0.86018648        nan        nan        nan 0.84498834 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648 0.84498834\n",
      "        nan 0.85109557        nan        nan 0.86018648        nan\n",
      "        nan 0.86018648 0.83892774 0.85109557 0.86018648        nan\n",
      " 0.84801865 0.84801865 0.84801865 0.86018648 0.83589744 0.86018648\n",
      " 0.86018648        nan        nan 0.85109557        nan        nan\n",
      " 0.86018648 0.84498834 0.83589744 0.84498834        nan        nan\n",
      " 0.84801865 0.83892774        nan 0.84801865 0.86018648 0.84801865\n",
      " 0.84498834 0.84498834 0.84498834 0.84498834        nan 0.86018648\n",
      "        nan 0.84498834 0.83892774        nan 0.86018648 0.84498834\n",
      " 0.84498834        nan 0.83897436 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.83589744        nan 0.84801865        nan\n",
      " 0.84498834        nan 0.86018648 0.84498834 0.86018648 0.84498834\n",
      " 0.85109557 0.86018648        nan 0.84498834 0.83892774 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648        nan\n",
      " 0.86018648 0.86018648 0.86018648        nan 0.84498834 0.84806527\n",
      " 0.86018648        nan 0.84806527 0.84498834 0.86018648 0.86018648\n",
      " 0.84801865        nan 0.86018648 0.86018648 0.83892774 0.86018648\n",
      " 0.84498834        nan        nan        nan        nan 0.83589744\n",
      " 0.86018648        nan 0.84498834        nan 0.86018648 0.86018648\n",
      " 0.84801865 0.84498834 0.86018648 0.84498834 0.86018648        nan\n",
      " 0.86018648        nan 0.83897436        nan 0.84498834        nan\n",
      " 0.83589744        nan        nan        nan 0.84498834        nan\n",
      " 0.84801865 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.84801865 0.84498834        nan 0.86018648 0.84498834 0.83589744\n",
      " 0.86018648 0.86018648 0.84801865        nan 0.84498834 0.83897436\n",
      " 0.83589744        nan        nan 0.86018648 0.84498834 0.84498834\n",
      "        nan 0.84498834 0.84806527 0.84498834 0.84801865 0.86018648\n",
      " 0.84806527        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018648 0.86018648 0.85109557 0.83892774\n",
      " 0.86018648 0.83589744        nan 0.86018648 0.84498834        nan\n",
      "        nan 0.84801865 0.84498834 0.84806527 0.84498834        nan\n",
      "        nan        nan 0.86018648 0.83589744 0.84801865 0.86018648\n",
      " 0.84498834 0.85109557        nan 0.86018648        nan 0.83589744\n",
      " 0.85109557 0.84498834        nan        nan 0.83589744        nan\n",
      " 0.86018648 0.84498834        nan 0.85109557 0.84498834 0.84498834\n",
      " 0.86018648        nan 0.86018648 0.86018648 0.86018648        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.83892774 0.83589744\n",
      " 0.84806527 0.83897436 0.84801865        nan 0.84498834 0.84498834\n",
      " 0.84801865        nan 0.86018648        nan 0.85109557 0.86018648\n",
      " 0.84498834        nan 0.84801865 0.83589744        nan        nan\n",
      " 0.84498834        nan        nan 0.83589744        nan 0.84498834\n",
      " 0.83897436 0.84801865        nan 0.86018648 0.83589744 0.84801865\n",
      " 0.84498834 0.84498834        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.86018648 0.86018648 0.86018648\n",
      " 0.84498834        nan 0.83892774        nan 0.86018648 0.85109557\n",
      " 0.83589744 0.86018648        nan        nan 0.84498834        nan\n",
      " 0.85109557        nan 0.86018648        nan        nan 0.84801865\n",
      " 0.84498834        nan 0.83589744 0.84806527 0.83589744        nan\n",
      " 0.86018648 0.83589744        nan 0.84498834        nan        nan\n",
      " 0.86018648        nan 0.84498834 0.84498834 0.84801865 0.86018648\n",
      "        nan        nan        nan 0.83589744        nan        nan\n",
      "        nan 0.84806527        nan        nan        nan 0.86018648\n",
      "        nan        nan 0.83892774 0.86018648        nan 0.86018648\n",
      " 0.83589744 0.86018648]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.86018262 0.86930234        nan 0.86930234 0.86018262 0.86094884\n",
      " 0.8617093  0.86550294 0.86930234        nan        nan        nan\n",
      "        nan 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86018262        nan 0.86018262 0.86094596        nan 0.86018551\n",
      " 0.86930234 0.86094596 0.86018262 0.86018262        nan 0.86018262\n",
      "        nan 0.85943081 0.8617093  0.8617093  0.86018551        nan\n",
      " 0.86930234 0.86018262 0.86018262 0.86018262        nan 0.86930234\n",
      "        nan 0.86018262        nan        nan        nan 0.86018262\n",
      "        nan        nan        nan 0.86018262 0.86018262        nan\n",
      "        nan 0.85487095        nan 0.86018551        nan        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.85487095 0.86930234 0.86018262\n",
      "        nan        nan 0.86550294 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86018262        nan 0.86094596        nan\n",
      "        nan        nan 0.86018551 0.86930234 0.85943081 0.86930234\n",
      " 0.86550294 0.86018262 0.86550294        nan        nan        nan\n",
      "        nan        nan 0.86094884 0.86930234        nan 0.8617093\n",
      "        nan 0.86094884 0.86018262 0.86550294 0.86930234 0.86018262\n",
      " 0.85943081        nan        nan 0.86550294        nan        nan\n",
      "        nan 0.86018262 0.86094596 0.86018551 0.86018262        nan\n",
      "        nan        nan 0.86930234 0.86930234        nan        nan\n",
      " 0.85943081 0.86930234        nan 0.86930234 0.86018262 0.86018262\n",
      " 0.86018262 0.86930234        nan 0.86930234        nan        nan\n",
      " 0.86930234        nan        nan 0.86550294 0.85943081 0.86930234\n",
      " 0.86018262        nan        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86018262 0.86094884        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86094884 0.86094596        nan 0.86930234 0.86094596\n",
      "        nan        nan 0.86018262        nan 0.86930234 0.86930234\n",
      " 0.85943081 0.86550294 0.85487095        nan        nan 0.86930234\n",
      " 0.86018262        nan        nan        nan 0.86550294 0.86930234\n",
      "        nan 0.86018262 0.8617093  0.86930234 0.86018262 0.86930234\n",
      "        nan 0.86094596        nan        nan 0.86018262        nan\n",
      "        nan 0.86018262 0.85943081 0.86094596 0.86018262        nan\n",
      " 0.86930234 0.86930234 0.86930234 0.86018262 0.86094884 0.86018262\n",
      " 0.86018262        nan        nan 0.86094596        nan        nan\n",
      " 0.86018262 0.86930234 0.86094884 0.86930234        nan        nan\n",
      " 0.86930234 0.85943081        nan 0.86930234 0.86018262 0.86930234\n",
      " 0.86930234 0.86930234 0.86930234 0.86550294        nan 0.86018262\n",
      "        nan 0.86930234 0.85943081        nan 0.86018262 0.86930234\n",
      " 0.86930234        nan 0.85487095 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86094884        nan 0.86930234        nan\n",
      " 0.86930234        nan 0.86018262 0.86930234 0.86018262 0.86930234\n",
      " 0.86094596 0.86018262        nan 0.86930234 0.85943081 0.86930234\n",
      "        nan 0.86018262 0.86094884 0.86930234 0.86018262        nan\n",
      " 0.86018262 0.86018262 0.86018262        nan 0.86550294 0.86018551\n",
      " 0.86018262        nan 0.86018551 0.86930234 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.86018262 0.86018262 0.85943081 0.86018262\n",
      " 0.86930234        nan        nan        nan        nan 0.86094884\n",
      " 0.86018262        nan 0.86930234        nan 0.86018262 0.86018262\n",
      " 0.86930234 0.86930234 0.86018262 0.86930234 0.86018262        nan\n",
      " 0.86018262        nan 0.85487095        nan 0.86930234        nan\n",
      " 0.8617093         nan        nan        nan 0.86550294        nan\n",
      " 0.86930234 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86930234 0.86930234        nan 0.86018262 0.86930234 0.86094884\n",
      " 0.86018262 0.86018262 0.86930234        nan 0.86930234 0.85487095\n",
      " 0.86094884        nan        nan 0.86018262 0.86550294 0.86550294\n",
      "        nan 0.86930234 0.86018551 0.86550294 0.86930234 0.86018262\n",
      " 0.86018551        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018262 0.86018262 0.86094596 0.85943081\n",
      " 0.86018262 0.86094884        nan 0.86018262 0.86550294        nan\n",
      "        nan 0.86930234 0.86930234 0.86018551 0.86930234        nan\n",
      "        nan        nan 0.86018262 0.86094884 0.86930234 0.86018262\n",
      " 0.86930234 0.86094596        nan 0.86018262        nan 0.8617093\n",
      " 0.86094596 0.86930234        nan        nan 0.8617093         nan\n",
      " 0.86018262 0.86550294        nan 0.86094596 0.86930234 0.86930234\n",
      " 0.86018262        nan 0.86018262 0.86018262 0.86018262        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.85943081 0.86094884\n",
      " 0.86018551 0.85487095 0.86930234        nan 0.86550294 0.86930234\n",
      " 0.86930234        nan 0.86018262        nan 0.86094596 0.86018262\n",
      " 0.86930234        nan 0.86930234 0.8617093         nan        nan\n",
      " 0.86550294        nan        nan 0.86094884        nan 0.86550294\n",
      " 0.85487095 0.86930234        nan 0.86018262 0.8617093  0.86930234\n",
      " 0.86550294 0.86930234        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.86018262 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.85943081        nan 0.86018262 0.86094596\n",
      " 0.8617093  0.86018262        nan        nan 0.86930234        nan\n",
      " 0.86094596        nan 0.86018262        nan        nan 0.86930234\n",
      " 0.86930234        nan 0.86094884 0.86018551 0.86094884        nan\n",
      " 0.86018262 0.86094884        nan 0.86550294        nan        nan\n",
      " 0.86018262        nan 0.86930234 0.86550294 0.86930234 0.86018262\n",
      "        nan        nan        nan 0.8617093         nan        nan\n",
      "        nan 0.86018551        nan        nan        nan 0.86018262\n",
      "        nan        nan 0.85943081 0.86018262        nan 0.86018262\n",
      " 0.86094884 0.86018262]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr_rs = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr_rs = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8297872 Precision=nan Recall=0.0000000 F1=0.0000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\AppData\\Local\\Temp\\ipykernel_37296\\520604890.py:6: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression usingÂ grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best accuracy score is 0.848018648018648\n",
      "... with parameters: {'C': 9, 'max_iter': 328, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "lr_gs =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lr_gs, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlr_gs = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM Classification by using Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'auto', 'degree': 1, 'coef0': 1, 'C': 90.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svm_poly_model_rs = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_poly_model_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestrecallsvm_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification by using Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best accuracy score is 0.8054079254079255\n",
      "... with parameters: {'C': 48.1, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecall = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decision tree by using Random search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'min_samples_split': 48, 'min_samples_leaf': 31, 'min_impurity_decrease': 0.0041, 'max_leaf_nodes': 14, 'max_depth': 30, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,50),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestdtree_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree by using Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 7, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "Dout_GS = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = Dout_GS, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestdtree_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8156028 Precision=0.2500000 Recall=0.0416667 F1=0.0714286\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5WfGTWb3hYd-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 972 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89       117\n",
      "           1       0.33      0.17      0.22        24\n",
      "\n",
      "    accuracy                           0.80       141\n",
      "   macro avg       0.59      0.55      0.55       141\n",
      "weighted avg       0.76      0.80      0.77       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Random SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.001, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (70,), 'alpha': 0.5, 'activation': 'logistic'}\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       117\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.83       141\n",
      "   macro avg       0.41      0.50      0.45       141\n",
      "weighted avg       0.69      0.83      0.75       141\n",
      "\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Grid SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "{'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (30,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.15, 'max_iter': 5000, 'solver': 'adam'}\n",
      "Wall time: 47.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       117\n",
      "           1       0.33      0.12      0.18        24\n",
      "\n",
      "    accuracy                           0.81       141\n",
      "   macro avg       0.59      0.54      0.54       141\n",
      "weighted avg       0.75      0.81      0.77       141\n",
      "\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a number of models, including logistic regression, SVM, decision trees, and neural network models for both random and grid search, to fit the Thoracic Surgery data in this code. We then assessed their performance using the accuracy of the metric. When we compare the accuracy scores of all other models, the decision tree model outperformed the logistic regression, svm, and neural network for both random and grid search models, with a better accuracy score of 86.01%, even though both logistic and svm random search are also with 86.01% as previously mentioned whereas neural networks scored only 83% for random and 81% for grid search, logistic regression and svm grid search have accuracy values of 84.80% and 80.54%, respectively, demonstrating that the decision tree model for both random and grid search fits best when compared to all the models for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
