{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression, SVM and DTree Classifer along with Neural Network and Deep Neural Netwrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup\n",
    "Import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from __future__ import print_function\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load data\n",
    "Load data (it's already cleaned and preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following snippet of code to debug problems with finding the .csv file path\n",
    "# This snippet of code will exit the program and print the current working directory.\n",
    "#import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"thoracic_train_X_risk.csv\")\n",
    "X_test = pd.read_csv(\"thoracic_test_X_risk.csv\")\n",
    "y_train = pd.read_csv(\"thoracic_train_y_risk.csv\")\n",
    "y_test = pd.read_csv(\"thoracic_test_y_risk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a dataframe to load the model performance metrics into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic regression using Randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 932, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "910 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "290 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.86018648 0.84801865        nan 0.84498834 0.86018648 0.83589744\n",
      " 0.83589744 0.84498834 0.84498834        nan        nan        nan\n",
      "        nan 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.86018648        nan 0.86018648 0.85109557        nan 0.84806527\n",
      " 0.84498834 0.85109557 0.86018648 0.86018648        nan 0.86018648\n",
      "        nan 0.83892774 0.83589744 0.83589744 0.84806527        nan\n",
      " 0.84801865 0.86018648 0.86018648 0.86018648        nan 0.84801865\n",
      "        nan 0.86018648        nan        nan        nan 0.86018648\n",
      "        nan        nan        nan 0.86018648 0.86018648        nan\n",
      "        nan 0.83897436        nan 0.84806527        nan        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.83897436 0.84498834 0.86018648\n",
      "        nan        nan 0.84498834 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.86018648        nan 0.85109557        nan\n",
      "        nan        nan 0.84806527 0.84498834 0.83892774 0.84498834\n",
      " 0.84498834 0.86018648 0.84498834        nan        nan        nan\n",
      "        nan        nan 0.83589744 0.84498834        nan 0.83589744\n",
      "        nan 0.83589744 0.86018648 0.84498834 0.84498834 0.86018648\n",
      " 0.83892774        nan        nan 0.84498834        nan        nan\n",
      "        nan 0.86018648 0.85109557 0.84806527 0.86018648        nan\n",
      "        nan        nan 0.84498834 0.84498834        nan        nan\n",
      " 0.83892774 0.84498834        nan 0.84801865 0.86018648 0.86018648\n",
      " 0.86018648 0.84498834        nan 0.84498834        nan        nan\n",
      " 0.84498834        nan        nan 0.84498834 0.83892774 0.84498834\n",
      " 0.86018648        nan        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.86018648 0.83589744        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.83589744 0.85109557        nan 0.84498834 0.85109557\n",
      "        nan        nan 0.86018648        nan 0.84498834 0.84498834\n",
      " 0.83892774 0.84498834 0.83897436        nan        nan 0.84498834\n",
      " 0.86018648        nan        nan        nan 0.84498834 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648 0.84498834\n",
      "        nan 0.85109557        nan        nan 0.86018648        nan\n",
      "        nan 0.86018648 0.83892774 0.85109557 0.86018648        nan\n",
      " 0.84801865 0.84801865 0.84801865 0.86018648 0.83589744 0.86018648\n",
      " 0.86018648        nan        nan 0.85109557        nan        nan\n",
      " 0.86018648 0.84498834 0.83589744 0.84498834        nan        nan\n",
      " 0.84801865 0.83892774        nan 0.84801865 0.86018648 0.84801865\n",
      " 0.84498834 0.84498834 0.84498834 0.84498834        nan 0.86018648\n",
      "        nan 0.84498834 0.83892774        nan 0.86018648 0.84498834\n",
      " 0.84498834        nan 0.83897436 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.83589744        nan 0.84801865        nan\n",
      " 0.84498834        nan 0.86018648 0.84498834 0.86018648 0.84498834\n",
      " 0.85109557 0.86018648        nan 0.84498834 0.83892774 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648        nan\n",
      " 0.86018648 0.86018648 0.86018648        nan 0.84498834 0.84806527\n",
      " 0.86018648        nan 0.84806527 0.84498834 0.86018648 0.86018648\n",
      " 0.84801865        nan 0.86018648 0.86018648 0.83892774 0.86018648\n",
      " 0.84498834        nan        nan        nan        nan 0.83589744\n",
      " 0.86018648        nan 0.84498834        nan 0.86018648 0.86018648\n",
      " 0.84801865 0.84498834 0.86018648 0.84498834 0.86018648        nan\n",
      " 0.86018648        nan 0.83897436        nan 0.84498834        nan\n",
      " 0.83589744        nan        nan        nan 0.84498834        nan\n",
      " 0.84801865 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.84801865 0.84498834        nan 0.86018648 0.84498834 0.83589744\n",
      " 0.86018648 0.86018648 0.84801865        nan 0.84498834 0.83897436\n",
      " 0.83589744        nan        nan 0.86018648 0.84498834 0.84498834\n",
      "        nan 0.84498834 0.84806527 0.84498834 0.84801865 0.86018648\n",
      " 0.84806527        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018648 0.86018648 0.85109557 0.83892774\n",
      " 0.86018648 0.83589744        nan 0.86018648 0.84498834        nan\n",
      "        nan 0.84801865 0.84498834 0.84806527 0.84498834        nan\n",
      "        nan        nan 0.86018648 0.83589744 0.84801865 0.86018648\n",
      " 0.84498834 0.85109557        nan 0.86018648        nan 0.83589744\n",
      " 0.85109557 0.84498834        nan        nan 0.83589744        nan\n",
      " 0.86018648 0.84498834        nan 0.85109557 0.84498834 0.84498834\n",
      " 0.86018648        nan 0.86018648 0.86018648 0.86018648        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.83892774 0.83589744\n",
      " 0.84806527 0.83897436 0.84801865        nan 0.84498834 0.84498834\n",
      " 0.84801865        nan 0.86018648        nan 0.85109557 0.86018648\n",
      " 0.84498834        nan 0.84801865 0.83589744        nan        nan\n",
      " 0.84498834        nan        nan 0.83589744        nan 0.84498834\n",
      " 0.83897436 0.84801865        nan 0.86018648 0.83589744 0.84801865\n",
      " 0.84498834 0.84498834        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.86018648 0.86018648 0.86018648\n",
      " 0.84498834        nan 0.83892774        nan 0.86018648 0.85109557\n",
      " 0.83589744 0.86018648        nan        nan 0.84498834        nan\n",
      " 0.85109557        nan 0.86018648        nan        nan 0.84801865\n",
      " 0.84498834        nan 0.83589744 0.84806527 0.83589744        nan\n",
      " 0.86018648 0.83589744        nan 0.84498834        nan        nan\n",
      " 0.86018648        nan 0.84498834 0.84498834 0.84801865 0.86018648\n",
      "        nan        nan        nan 0.83589744        nan        nan\n",
      "        nan 0.84806527        nan        nan        nan 0.86018648\n",
      "        nan        nan 0.83892774 0.86018648        nan 0.86018648\n",
      " 0.83589744 0.86018648]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.86018262 0.86930234        nan 0.86930234 0.86018262 0.86094884\n",
      " 0.8617093  0.86550294 0.86930234        nan        nan        nan\n",
      "        nan 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86018262        nan 0.86018262 0.86094596        nan 0.86018551\n",
      " 0.86930234 0.86094596 0.86018262 0.86018262        nan 0.86018262\n",
      "        nan 0.85943081 0.8617093  0.8617093  0.86018551        nan\n",
      " 0.86930234 0.86018262 0.86018262 0.86018262        nan 0.86930234\n",
      "        nan 0.86018262        nan        nan        nan 0.86018262\n",
      "        nan        nan        nan 0.86018262 0.86018262        nan\n",
      "        nan 0.85487095        nan 0.86018551        nan        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.85487095 0.86930234 0.86018262\n",
      "        nan        nan 0.86550294 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86018262        nan 0.86094596        nan\n",
      "        nan        nan 0.86018551 0.86930234 0.85943081 0.86930234\n",
      " 0.86550294 0.86018262 0.86550294        nan        nan        nan\n",
      "        nan        nan 0.86094884 0.86930234        nan 0.8617093\n",
      "        nan 0.86094884 0.86018262 0.86550294 0.86930234 0.86018262\n",
      " 0.85943081        nan        nan 0.86550294        nan        nan\n",
      "        nan 0.86018262 0.86094596 0.86018551 0.86018262        nan\n",
      "        nan        nan 0.86930234 0.86930234        nan        nan\n",
      " 0.85943081 0.86930234        nan 0.86930234 0.86018262 0.86018262\n",
      " 0.86018262 0.86930234        nan 0.86930234        nan        nan\n",
      " 0.86930234        nan        nan 0.86550294 0.85943081 0.86930234\n",
      " 0.86018262        nan        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86018262 0.86094884        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86094884 0.86094596        nan 0.86930234 0.86094596\n",
      "        nan        nan 0.86018262        nan 0.86930234 0.86930234\n",
      " 0.85943081 0.86550294 0.85487095        nan        nan 0.86930234\n",
      " 0.86018262        nan        nan        nan 0.86550294 0.86930234\n",
      "        nan 0.86018262 0.8617093  0.86930234 0.86018262 0.86930234\n",
      "        nan 0.86094596        nan        nan 0.86018262        nan\n",
      "        nan 0.86018262 0.85943081 0.86094596 0.86018262        nan\n",
      " 0.86930234 0.86930234 0.86930234 0.86018262 0.86094884 0.86018262\n",
      " 0.86018262        nan        nan 0.86094596        nan        nan\n",
      " 0.86018262 0.86930234 0.86094884 0.86930234        nan        nan\n",
      " 0.86930234 0.85943081        nan 0.86930234 0.86018262 0.86930234\n",
      " 0.86930234 0.86930234 0.86930234 0.86550294        nan 0.86018262\n",
      "        nan 0.86930234 0.85943081        nan 0.86018262 0.86930234\n",
      " 0.86930234        nan 0.85487095 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86094884        nan 0.86930234        nan\n",
      " 0.86930234        nan 0.86018262 0.86930234 0.86018262 0.86930234\n",
      " 0.86094596 0.86018262        nan 0.86930234 0.85943081 0.86930234\n",
      "        nan 0.86018262 0.86094884 0.86930234 0.86018262        nan\n",
      " 0.86018262 0.86018262 0.86018262        nan 0.86550294 0.86018551\n",
      " 0.86018262        nan 0.86018551 0.86930234 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.86018262 0.86018262 0.85943081 0.86018262\n",
      " 0.86930234        nan        nan        nan        nan 0.86094884\n",
      " 0.86018262        nan 0.86930234        nan 0.86018262 0.86018262\n",
      " 0.86930234 0.86930234 0.86018262 0.86930234 0.86018262        nan\n",
      " 0.86018262        nan 0.85487095        nan 0.86930234        nan\n",
      " 0.8617093         nan        nan        nan 0.86550294        nan\n",
      " 0.86930234 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86930234 0.86930234        nan 0.86018262 0.86930234 0.86094884\n",
      " 0.86018262 0.86018262 0.86930234        nan 0.86930234 0.85487095\n",
      " 0.86094884        nan        nan 0.86018262 0.86550294 0.86550294\n",
      "        nan 0.86930234 0.86018551 0.86550294 0.86930234 0.86018262\n",
      " 0.86018551        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018262 0.86018262 0.86094596 0.85943081\n",
      " 0.86018262 0.86094884        nan 0.86018262 0.86550294        nan\n",
      "        nan 0.86930234 0.86930234 0.86018551 0.86930234        nan\n",
      "        nan        nan 0.86018262 0.86094884 0.86930234 0.86018262\n",
      " 0.86930234 0.86094596        nan 0.86018262        nan 0.8617093\n",
      " 0.86094596 0.86930234        nan        nan 0.8617093         nan\n",
      " 0.86018262 0.86550294        nan 0.86094596 0.86930234 0.86930234\n",
      " 0.86018262        nan 0.86018262 0.86018262 0.86018262        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.85943081 0.86094884\n",
      " 0.86018551 0.85487095 0.86930234        nan 0.86550294 0.86930234\n",
      " 0.86930234        nan 0.86018262        nan 0.86094596 0.86018262\n",
      " 0.86930234        nan 0.86930234 0.8617093         nan        nan\n",
      " 0.86550294        nan        nan 0.86094884        nan 0.86550294\n",
      " 0.85487095 0.86930234        nan 0.86018262 0.8617093  0.86930234\n",
      " 0.86550294 0.86930234        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.86018262 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.85943081        nan 0.86018262 0.86094596\n",
      " 0.8617093  0.86018262        nan        nan 0.86930234        nan\n",
      " 0.86094596        nan 0.86018262        nan        nan 0.86930234\n",
      " 0.86930234        nan 0.86094884 0.86018551 0.86094884        nan\n",
      " 0.86018262 0.86094884        nan 0.86550294        nan        nan\n",
      " 0.86018262        nan 0.86930234 0.86550294 0.86930234 0.86018262\n",
      "        nan        nan        nan 0.8617093         nan        nan\n",
      "        nan 0.86018551        nan        nan        nan 0.86018262\n",
      "        nan        nan 0.85943081 0.86018262        nan 0.86018262\n",
      " 0.86094884 0.86018262]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr_rs = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr_rs = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8297872 Precision=nan Recall=0.0000000 F1=0.0000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\AppData\\Local\\Temp\\ipykernel_37296\\520604890.py:6: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression usingÂ grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best accuracy score is 0.848018648018648\n",
      "... with parameters: {'C': 9, 'max_iter': 328, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "lr_gs =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lr_gs, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlr_gs = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM Classification by using Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'auto', 'degree': 1, 'coef0': 1, 'C': 90.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svm_poly_model_rs = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_poly_model_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestrecallsvm_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification by using Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best accuracy score is 0.8054079254079255\n",
      "... with parameters: {'C': 48.1, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecall = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Decision tree by using Random search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'min_samples_split': 48, 'min_samples_leaf': 31, 'min_impurity_decrease': 0.0041, 'max_leaf_nodes': 14, 'max_depth': 30, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,50),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestdtree_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree by using Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 7, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "Dout_GS = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = Dout_GS, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestdtree_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8156028 Precision=0.2500000 Recall=0.0416667 F1=0.0714286\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5WfGTWb3hYd-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 972 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ann = MLPClassifier(hidden_layer_sizes=(60,50,40), solver='adam', max_iter=200)\n",
    "_ = ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = ann.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89       117\n",
      "           1       0.33      0.17      0.22        24\n",
      "\n",
      "    accuracy                           0.80       141\n",
      "   macro avg       0.59      0.55      0.55       141\n",
      "weighted avg       0.76      0.80      0.77       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Random SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'adam', 'max_iter': 5000, 'learning_rate_init': 0.001, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (70,), 'alpha': 0.5, 'activation': 'logistic'}\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (50,), (70,),(50,30), (40,20), (60,40, 20), (70,50,40)],\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0, .2, .5, .7, 1],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = RandomizedSearchCV(estimator = ann, param_distributions=param_grid, cv=kfolds, n_iter=100,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       117\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.83       141\n",
      "   macro avg       0.41      0.50      0.45       141\n",
      "weighted avg       0.69      0.83      0.75       141\n",
      "\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network With Grid SearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "{'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (30,), 'learning_rate': 'invscaling', 'learning_rate_init': 0.15, 'max_iter': 5000, 'solver': 'adam'}\n",
      "Wall time: 47.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1096: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [ (30,), (50,), (70,), (90,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.5, .7, 1],\n",
    "    'learning_rate': ['adaptive', 'invscaling'],\n",
    "    'learning_rate_init': [0.005, 0.01, 0.15],\n",
    "    'max_iter': [5000]\n",
    "}\n",
    "\n",
    "ann = MLPClassifier()\n",
    "grid_search = GridSearchCV(estimator = ann, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_\n",
    "\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       117\n",
      "           1       0.33      0.12      0.18        24\n",
      "\n",
      "    accuracy                           0.81       141\n",
      "   macro avg       0.59      0.54      0.54       141\n",
      "weighted avg       0.75      0.81      0.77       141\n",
      "\n",
      "Wall time: 11 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = bestRecallTree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 62.5 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create model stucture\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Input(17))\n",
    "model.add(keras.layers.Dense(17, activation='relu'))\n",
    "model.add(keras.layers.Dense(17, activation='relu'))\n",
    "model.add(keras.layers.Dense(17, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) # final layer, 10 categories\n",
    "\n",
    "\n",
    "# compile\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# if you want to overide the defaults for the optimizer....\n",
    "#adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 1s 118ms/step - loss: 0.5867 - accuracy: 0.7781 - val_loss: 0.5631 - val_accuracy: 0.8014\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5601 - accuracy: 0.8176 - val_loss: 0.5446 - val_accuracy: 0.8298\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5370 - accuracy: 0.8541 - val_loss: 0.5297 - val_accuracy: 0.8298\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5196 - accuracy: 0.8602 - val_loss: 0.5179 - val_accuracy: 0.8298\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5042 - accuracy: 0.8602 - val_loss: 0.5082 - val_accuracy: 0.8298\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4901 - accuracy: 0.8602 - val_loss: 0.5011 - val_accuracy: 0.8298\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4793 - accuracy: 0.8602 - val_loss: 0.4950 - val_accuracy: 0.8298\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4700 - accuracy: 0.8602 - val_loss: 0.4904 - val_accuracy: 0.8298\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4611 - accuracy: 0.8602 - val_loss: 0.4870 - val_accuracy: 0.8298\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4526 - accuracy: 0.8602 - val_loss: 0.4845 - val_accuracy: 0.8298\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4463 - accuracy: 0.8602 - val_loss: 0.4827 - val_accuracy: 0.8298\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4397 - accuracy: 0.8602 - val_loss: 0.4815 - val_accuracy: 0.8298\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4342 - accuracy: 0.8602 - val_loss: 0.4808 - val_accuracy: 0.8298\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4288 - accuracy: 0.8602 - val_loss: 0.4801 - val_accuracy: 0.8298\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4239 - accuracy: 0.8602 - val_loss: 0.4793 - val_accuracy: 0.8298\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4194 - accuracy: 0.8602 - val_loss: 0.4787 - val_accuracy: 0.8298\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4148 - accuracy: 0.8602 - val_loss: 0.4784 - val_accuracy: 0.8298\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4107 - accuracy: 0.8602 - val_loss: 0.4779 - val_accuracy: 0.8298\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4068 - accuracy: 0.8602 - val_loss: 0.4773 - val_accuracy: 0.8298\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4030 - accuracy: 0.8602 - val_loss: 0.4765 - val_accuracy: 0.8298\n",
      "CPU times: total: 5.97 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=20, batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4764949679374695, 0.8297872543334961]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.48\n",
      "accuracy: 82.98%\n"
     ]
    }
   ],
   "source": [
    "# let's format this into a better output...\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide and Deep Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define the model: for multi-class\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=17))\n",
    "model.add(keras.layers.Dense(75, activation='relu'))\n",
    "model.add(keras.layers.Dense(75, activation='relu'))\n",
    "model.add(keras.layers.Dense(75, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "#Optimizer:\n",
    "adam = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 1s 82ms/step - loss: 0.5444 - accuracy: 0.7447 - val_loss: 0.5153 - val_accuracy: 0.8298\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3885 - accuracy: 0.8602 - val_loss: 0.4460 - val_accuracy: 0.8298\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3601 - accuracy: 0.8663 - val_loss: 0.4824 - val_accuracy: 0.8227\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3195 - accuracy: 0.8723 - val_loss: 0.5608 - val_accuracy: 0.8227\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2899 - accuracy: 0.8693 - val_loss: 0.5105 - val_accuracy: 0.8156\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2827 - accuracy: 0.8815 - val_loss: 0.5331 - val_accuracy: 0.8156\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2526 - accuracy: 0.8845 - val_loss: 0.6096 - val_accuracy: 0.8156\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2386 - accuracy: 0.8936 - val_loss: 0.6099 - val_accuracy: 0.8014\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2300 - accuracy: 0.9149 - val_loss: 0.6736 - val_accuracy: 0.8085\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2015 - accuracy: 0.9119 - val_loss: 0.7643 - val_accuracy: 0.8227\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1785 - accuracy: 0.9210 - val_loss: 0.7147 - val_accuracy: 0.8014\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1580 - accuracy: 0.9544 - val_loss: 0.7744 - val_accuracy: 0.8014\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1412 - accuracy: 0.9422 - val_loss: 0.8689 - val_accuracy: 0.8227\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1108 - accuracy: 0.9605 - val_loss: 0.9451 - val_accuracy: 0.7872\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0936 - accuracy: 0.9635 - val_loss: 1.1103 - val_accuracy: 0.7730\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0803 - accuracy: 0.9696 - val_loss: 1.2438 - val_accuracy: 0.8014\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0868 - accuracy: 0.9696 - val_loss: 1.2559 - val_accuracy: 0.7660\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.0642 - accuracy: 0.9757 - val_loss: 1.3442 - val_accuracy: 0.7376\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.1006 - accuracy: 0.9574 - val_loss: 1.6086 - val_accuracy: 0.8014\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0913 - accuracy: 0.9635 - val_loss: 1.4741 - val_accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4741122722625732, 0.7730496525764465]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "scores\n",
    "\n",
    "# In results, first is loss, second is accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.47\n",
      "accuracy: 77.30%\n"
     ]
    }
   ],
   "source": [
    "# extract the accuracy from model.evaluate\n",
    "\n",
    "print(\"%s: %.2f\" % (model.metrics_names[0], scores[0]))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomGridSearch with Glorot Normal Intializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# If you don't have the following installed, from command line '!pip install scikeras'\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import GlorotNormal\n",
    "\n",
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "def build_clf(hidden_layer_sizes, dropout):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(keras.layers.Input(shape=17)),\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        model.add(keras.layers.Dense(hidden_layer_size, kernel_initializer= tf.keras.initializers.GlorotNormal(), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), activation=\"relu\"))\n",
    "        model.add(keras.layers.Dropout(dropout))\n",
    "    ann.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    ann.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "keras_clf = KerasClassifier(\n",
    "    model=build_clf,\n",
    "    hidden_layer_sizes=64,\n",
    "    dropout = 0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs', 'hidden_layer_sizes', 'dropout', 'class_weight'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    'optimizer__learning_rate': [0.0005, 0.001, 0.005],\n",
    "    'model__hidden_layer_sizes': [(70,),(90, ), (100,), (100, 90)],\n",
    "    'model__dropout': [0, 0.1],\n",
    "    'batch_size':[20, 60, 100],\n",
    "    'epochs':[10, 50, 100],\n",
    "    'optimizer':[\"adam\",'sgd']\n",
    "}\n",
    "keras_clf.get_params().keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(estimator=keras_clf, param_distributions=params, scoring='accuracy', n_iter=50, cv=5)\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000) # note: the default is 3000 (python 3.9)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "callback = [earlystop]\n",
    "\n",
    "_ = rnd_search_cv.fit(X_train, y_train, callbacks=callback, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer__learning_rate': 0.001,\n",
       " 'optimizer': 'sgd',\n",
       " 'model__hidden_layer_sizes': (100,),\n",
       " 'model__dropout': 0.1,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 20}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer__learning_rate': 0.001, 'optimizer': 'sgd', 'model__hidden_layer_sizes': (100,), 'model__dropout': 0.1, 'epochs': 100, 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "best_net = rnd_search_cv.best_estimator_\n",
    "print(rnd_search_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91       117\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.83       141\n",
      "   macro avg       0.41      0.50      0.45       141\n",
      "weighted avg       0.69      0.83      0.75       141\n",
      "\n",
      "CPU times: total: 172 ms\n",
      "Wall time: 156 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hiran\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = best_net.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis we mainly compare the MLP classifier of neural network and keras model for wide and deep neural network, in which for activation function we used sigmoid and for the loss function I had used binary_crossentropy, as the dataset I have choosen is a binary classified datasets.\n",
    "\n",
    "WithinÂ this code, we used a number of different models to match the data from thoracic surgery, such as logistic regression, SVM, decision trees, and neural network models and keras technique for wide and deep neural networkÂ for both random and grid search. Using the metric's accuracy, we then evaluated their performance. \n",
    "\n",
    "When we compare the accuracy scores of all other models, the decision tree model outperformed the logistic regression, svm, and neural network for both random and grid search models, with a better accuracy score of 86.01%. However, both logistic and svm random search are also with 86.01% as previously mentioned. \n",
    "\n",
    "Whereas neural networks scored 83% for random and 81% for grid search and in the keras model for Wide and Deep Network achieved only an accuracy of 77.03% and the Deep Network along with kernal intializer achieved an accuracy of 83% showing that, of all the models for this dataset, the decision tree model for both random and grid search fits best for the dataset.\n",
    "\n",
    "However it is important to remember that along with the dataset choosen, the optimization algorithm and loss function that is chosen, as well as hyperparameters like the network's layer and nodes, can all affect how well a neural network performs. As a result, it is important to assess how well the neural network is performing and make any necessary adjustments to the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
