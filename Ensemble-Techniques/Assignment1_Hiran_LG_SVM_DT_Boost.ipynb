{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression, SVM and DTree Classifer\n",
    "\n",
    "In this we will demonstarte how to use Logistic Regression in scikit-learn to perfrom logistic regression\n",
    "\n",
    "In this tutorial we will demonstrate how to use the SVM class in scikit-learn to perform logistic regression on a dataset.\n",
    "\n",
    "In this tutorial we will demonstrate how to use the `DecisionTreeClassifer` class in `scikit-learn` and we use boosting techniques to perform classifications predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Setup\n",
    "Import modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load data\n",
    "Load data (it's already cleaned and preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following snippet of code to debug problems with finding the .csv file path\n",
    "# This snippet of code will exit the program and print the current working directory.\n",
    "#import os\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"thoracic_train_X_risk.csv\")\n",
    "X_test = pd.read_csv(\"thoracic_test_X_risk.csv\")\n",
    "y_train = pd.read_csv(\"thoracic_train_y_risk.csv\")\n",
    "y_test = pd.read_csv(\"thoracic_test_y_risk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a dataframe to load the model performance metrics into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using Randomsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'solver': 'saga', 'penalty': 'l1', 'max_iter': 932, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "910 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "290 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.86018648 0.84801865        nan 0.84498834 0.86018648 0.83589744\n",
      " 0.83589744 0.84498834 0.84498834        nan        nan        nan\n",
      "        nan 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.86018648        nan 0.86018648 0.85109557        nan 0.84806527\n",
      " 0.84498834 0.85109557 0.86018648 0.86018648        nan 0.86018648\n",
      "        nan 0.83892774 0.83589744 0.83589744 0.84806527        nan\n",
      " 0.84801865 0.86018648 0.86018648 0.86018648        nan 0.84801865\n",
      "        nan 0.86018648        nan        nan        nan 0.86018648\n",
      "        nan        nan        nan 0.86018648 0.86018648        nan\n",
      "        nan 0.83897436        nan 0.84806527        nan        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.83897436 0.84498834 0.86018648\n",
      "        nan        nan 0.84498834 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.86018648        nan 0.85109557        nan\n",
      "        nan        nan 0.84806527 0.84498834 0.83892774 0.84498834\n",
      " 0.84498834 0.86018648 0.84498834        nan        nan        nan\n",
      "        nan        nan 0.83589744 0.84498834        nan 0.83589744\n",
      "        nan 0.83589744 0.86018648 0.84498834 0.84498834 0.86018648\n",
      " 0.83892774        nan        nan 0.84498834        nan        nan\n",
      "        nan 0.86018648 0.85109557 0.84806527 0.86018648        nan\n",
      "        nan        nan 0.84498834 0.84498834        nan        nan\n",
      " 0.83892774 0.84498834        nan 0.84801865 0.86018648 0.86018648\n",
      " 0.86018648 0.84498834        nan 0.84498834        nan        nan\n",
      " 0.84498834        nan        nan 0.84498834 0.83892774 0.84498834\n",
      " 0.86018648        nan        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.86018648 0.83589744        nan 0.84498834 0.84498834\n",
      " 0.86018648 0.83589744 0.85109557        nan 0.84498834 0.85109557\n",
      "        nan        nan 0.86018648        nan 0.84498834 0.84498834\n",
      " 0.83892774 0.84498834 0.83897436        nan        nan 0.84498834\n",
      " 0.86018648        nan        nan        nan 0.84498834 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648 0.84498834\n",
      "        nan 0.85109557        nan        nan 0.86018648        nan\n",
      "        nan 0.86018648 0.83892774 0.85109557 0.86018648        nan\n",
      " 0.84801865 0.84801865 0.84801865 0.86018648 0.83589744 0.86018648\n",
      " 0.86018648        nan        nan 0.85109557        nan        nan\n",
      " 0.86018648 0.84498834 0.83589744 0.84498834        nan        nan\n",
      " 0.84801865 0.83892774        nan 0.84801865 0.86018648 0.84801865\n",
      " 0.84498834 0.84498834 0.84498834 0.84498834        nan 0.86018648\n",
      "        nan 0.84498834 0.83892774        nan 0.86018648 0.84498834\n",
      " 0.84498834        nan 0.83897436 0.86018648 0.86018648 0.86018648\n",
      "        nan 0.84498834 0.83589744        nan 0.84801865        nan\n",
      " 0.84498834        nan 0.86018648 0.84498834 0.86018648 0.84498834\n",
      " 0.85109557 0.86018648        nan 0.84498834 0.83892774 0.84498834\n",
      "        nan 0.86018648 0.83589744 0.84498834 0.86018648        nan\n",
      " 0.86018648 0.86018648 0.86018648        nan 0.84498834 0.84806527\n",
      " 0.86018648        nan 0.84806527 0.84498834 0.86018648 0.86018648\n",
      " 0.84801865        nan 0.86018648 0.86018648 0.83892774 0.86018648\n",
      " 0.84498834        nan        nan        nan        nan 0.83589744\n",
      " 0.86018648        nan 0.84498834        nan 0.86018648 0.86018648\n",
      " 0.84801865 0.84498834 0.86018648 0.84498834 0.86018648        nan\n",
      " 0.86018648        nan 0.83897436        nan 0.84498834        nan\n",
      " 0.83589744        nan        nan        nan 0.84498834        nan\n",
      " 0.84801865 0.86018648        nan 0.86018648        nan        nan\n",
      " 0.84801865 0.84498834        nan 0.86018648 0.84498834 0.83589744\n",
      " 0.86018648 0.86018648 0.84801865        nan 0.84498834 0.83897436\n",
      " 0.83589744        nan        nan 0.86018648 0.84498834 0.84498834\n",
      "        nan 0.84498834 0.84806527 0.84498834 0.84801865 0.86018648\n",
      " 0.84806527        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018648 0.86018648 0.85109557 0.83892774\n",
      " 0.86018648 0.83589744        nan 0.86018648 0.84498834        nan\n",
      "        nan 0.84801865 0.84498834 0.84806527 0.84498834        nan\n",
      "        nan        nan 0.86018648 0.83589744 0.84801865 0.86018648\n",
      " 0.84498834 0.85109557        nan 0.86018648        nan 0.83589744\n",
      " 0.85109557 0.84498834        nan        nan 0.83589744        nan\n",
      " 0.86018648 0.84498834        nan 0.85109557 0.84498834 0.84498834\n",
      " 0.86018648        nan 0.86018648 0.86018648 0.86018648        nan\n",
      " 0.86018648 0.84498834        nan        nan 0.83892774 0.83589744\n",
      " 0.84806527 0.83897436 0.84801865        nan 0.84498834 0.84498834\n",
      " 0.84801865        nan 0.86018648        nan 0.85109557 0.86018648\n",
      " 0.84498834        nan 0.84801865 0.83589744        nan        nan\n",
      " 0.84498834        nan        nan 0.83589744        nan 0.84498834\n",
      " 0.83897436 0.84801865        nan 0.86018648 0.83589744 0.84801865\n",
      " 0.84498834 0.84498834        nan 0.84498834        nan 0.84498834\n",
      " 0.86018648 0.84498834 0.84498834 0.86018648 0.86018648 0.86018648\n",
      " 0.84498834        nan 0.83892774        nan 0.86018648 0.85109557\n",
      " 0.83589744 0.86018648        nan        nan 0.84498834        nan\n",
      " 0.85109557        nan 0.86018648        nan        nan 0.84801865\n",
      " 0.84498834        nan 0.83589744 0.84806527 0.83589744        nan\n",
      " 0.86018648 0.83589744        nan 0.84498834        nan        nan\n",
      " 0.86018648        nan 0.84498834 0.84498834 0.84801865 0.86018648\n",
      "        nan        nan        nan 0.83589744        nan        nan\n",
      "        nan 0.84806527        nan        nan        nan 0.86018648\n",
      "        nan        nan 0.83892774 0.86018648        nan 0.86018648\n",
      " 0.83589744 0.86018648]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.86018262 0.86930234        nan 0.86930234 0.86018262 0.86094884\n",
      " 0.8617093  0.86550294 0.86930234        nan        nan        nan\n",
      "        nan 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86018262        nan 0.86018262 0.86094596        nan 0.86018551\n",
      " 0.86930234 0.86094596 0.86018262 0.86018262        nan 0.86018262\n",
      "        nan 0.85943081 0.8617093  0.8617093  0.86018551        nan\n",
      " 0.86930234 0.86018262 0.86018262 0.86018262        nan 0.86930234\n",
      "        nan 0.86018262        nan        nan        nan 0.86018262\n",
      "        nan        nan        nan 0.86018262 0.86018262        nan\n",
      "        nan 0.85487095        nan 0.86018551        nan        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.85487095 0.86930234 0.86018262\n",
      "        nan        nan 0.86550294 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86018262        nan 0.86094596        nan\n",
      "        nan        nan 0.86018551 0.86930234 0.85943081 0.86930234\n",
      " 0.86550294 0.86018262 0.86550294        nan        nan        nan\n",
      "        nan        nan 0.86094884 0.86930234        nan 0.8617093\n",
      "        nan 0.86094884 0.86018262 0.86550294 0.86930234 0.86018262\n",
      " 0.85943081        nan        nan 0.86550294        nan        nan\n",
      "        nan 0.86018262 0.86094596 0.86018551 0.86018262        nan\n",
      "        nan        nan 0.86930234 0.86930234        nan        nan\n",
      " 0.85943081 0.86930234        nan 0.86930234 0.86018262 0.86018262\n",
      " 0.86018262 0.86930234        nan 0.86930234        nan        nan\n",
      " 0.86930234        nan        nan 0.86550294 0.85943081 0.86930234\n",
      " 0.86018262        nan        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86018262 0.86094884        nan 0.86930234 0.86930234\n",
      " 0.86018262 0.86094884 0.86094596        nan 0.86930234 0.86094596\n",
      "        nan        nan 0.86018262        nan 0.86930234 0.86930234\n",
      " 0.85943081 0.86550294 0.85487095        nan        nan 0.86930234\n",
      " 0.86018262        nan        nan        nan 0.86550294 0.86930234\n",
      "        nan 0.86018262 0.8617093  0.86930234 0.86018262 0.86930234\n",
      "        nan 0.86094596        nan        nan 0.86018262        nan\n",
      "        nan 0.86018262 0.85943081 0.86094596 0.86018262        nan\n",
      " 0.86930234 0.86930234 0.86930234 0.86018262 0.86094884 0.86018262\n",
      " 0.86018262        nan        nan 0.86094596        nan        nan\n",
      " 0.86018262 0.86930234 0.86094884 0.86930234        nan        nan\n",
      " 0.86930234 0.85943081        nan 0.86930234 0.86018262 0.86930234\n",
      " 0.86930234 0.86930234 0.86930234 0.86550294        nan 0.86018262\n",
      "        nan 0.86930234 0.85943081        nan 0.86018262 0.86930234\n",
      " 0.86930234        nan 0.85487095 0.86018262 0.86018262 0.86018262\n",
      "        nan 0.86930234 0.86094884        nan 0.86930234        nan\n",
      " 0.86930234        nan 0.86018262 0.86930234 0.86018262 0.86930234\n",
      " 0.86094596 0.86018262        nan 0.86930234 0.85943081 0.86930234\n",
      "        nan 0.86018262 0.86094884 0.86930234 0.86018262        nan\n",
      " 0.86018262 0.86018262 0.86018262        nan 0.86550294 0.86018551\n",
      " 0.86018262        nan 0.86018551 0.86930234 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.86018262 0.86018262 0.85943081 0.86018262\n",
      " 0.86930234        nan        nan        nan        nan 0.86094884\n",
      " 0.86018262        nan 0.86930234        nan 0.86018262 0.86018262\n",
      " 0.86930234 0.86930234 0.86018262 0.86930234 0.86018262        nan\n",
      " 0.86018262        nan 0.85487095        nan 0.86930234        nan\n",
      " 0.8617093         nan        nan        nan 0.86550294        nan\n",
      " 0.86930234 0.86018262        nan 0.86018262        nan        nan\n",
      " 0.86930234 0.86930234        nan 0.86018262 0.86930234 0.86094884\n",
      " 0.86018262 0.86018262 0.86930234        nan 0.86930234 0.85487095\n",
      " 0.86094884        nan        nan 0.86018262 0.86550294 0.86550294\n",
      "        nan 0.86930234 0.86018551 0.86550294 0.86930234 0.86018262\n",
      " 0.86018551        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.86018262 0.86018262 0.86094596 0.85943081\n",
      " 0.86018262 0.86094884        nan 0.86018262 0.86550294        nan\n",
      "        nan 0.86930234 0.86930234 0.86018551 0.86930234        nan\n",
      "        nan        nan 0.86018262 0.86094884 0.86930234 0.86018262\n",
      " 0.86930234 0.86094596        nan 0.86018262        nan 0.8617093\n",
      " 0.86094596 0.86930234        nan        nan 0.8617093         nan\n",
      " 0.86018262 0.86550294        nan 0.86094596 0.86930234 0.86930234\n",
      " 0.86018262        nan 0.86018262 0.86018262 0.86018262        nan\n",
      " 0.86018262 0.86930234        nan        nan 0.85943081 0.86094884\n",
      " 0.86018551 0.85487095 0.86930234        nan 0.86550294 0.86930234\n",
      " 0.86930234        nan 0.86018262        nan 0.86094596 0.86018262\n",
      " 0.86930234        nan 0.86930234 0.8617093         nan        nan\n",
      " 0.86550294        nan        nan 0.86094884        nan 0.86550294\n",
      " 0.85487095 0.86930234        nan 0.86018262 0.8617093  0.86930234\n",
      " 0.86550294 0.86930234        nan 0.86930234        nan 0.86930234\n",
      " 0.86018262 0.86930234 0.86930234 0.86018262 0.86018262 0.86018262\n",
      " 0.86930234        nan 0.85943081        nan 0.86018262 0.86094596\n",
      " 0.8617093  0.86018262        nan        nan 0.86930234        nan\n",
      " 0.86094596        nan 0.86018262        nan        nan 0.86930234\n",
      " 0.86930234        nan 0.86094884 0.86018551 0.86094884        nan\n",
      " 0.86018262 0.86094884        nan 0.86550294        nan        nan\n",
      " 0.86018262        nan 0.86930234 0.86550294 0.86930234 0.86018262\n",
      "        nan        nan        nan 0.8617093         nan        nan\n",
      "        nan 0.86018551        nan        nan        nan 0.86018262\n",
      "        nan        nan 0.85943081 0.86018262        nan 0.86018262\n",
      " 0.86094884 0.86018262]\n",
      "  warnings.warn(\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr_rs = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr_rs = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8297872 Precision=nan Recall=0.0000000 F1=0.0000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\AppData\\Local\\Temp\\ipykernel_37296\\520604890.py:6: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, rand_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best accuracy score is 0.848018648018648\n",
      "... with parameters: {'C': 9, 'max_iter': 328, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "lr_gs =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = lr_gs, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlr_gs = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM Classification by using Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'auto', 'degree': 1, 'coef0': 1, 'C': 90.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,100,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "svm_poly_model_rs = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_poly_model_rs, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestrecallsvm_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8226950 Precision=0.0000000 Recall=0.0000000 F1=0.0000000\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification by using Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best accuracy score is 0.8054079254079255\n",
      "... with parameters: {'C': 48.1, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hiran\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecall = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree by using Random search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'min_samples_split': 48, 'min_samples_leaf': 31, 'min_impurity_decrease': 0.0041, 'max_leaf_nodes': 14, 'max_depth': 30, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,50),  \n",
    "    'min_samples_leaf': np.arange(1,50),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 50), \n",
    "    'max_depth': np.arange(1,50), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestdtree_rand = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.7730496 Precision=0.2142857 Recall=0.1250000 F1=0.1578947\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree by using Grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9072 candidates, totalling 45360 fits\n",
      "The best accuracy score is 0.8601864801864803\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 15, 'max_leaf_nodes': 162, 'min_impurity_decrease': 0.0048, 'min_samples_leaf': 7, 'min_samples_split': 30}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"accuracy\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(30,36),  \n",
    "    'min_samples_leaf': np.arange(6,12),\n",
    "    'min_impurity_decrease': np.arange(0.0048, 0.0054, 0.0001),\n",
    "    'max_leaf_nodes': np.arange(162,168), \n",
    "    'max_depth': np.arange(15,21), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "Dout_GS = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = Dout_GS, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestdtree_grid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.8156028 Precision=0.2500000 Recall=0.0416667 F1=0.0714286\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the often used evaluation metric to evaluate how well a model performs on a dataset is accuracy. When the dataset has a balanced distribution of classes, which means that each class appears in the dataset with nearly the same frequency, it is usually used and most of the data in my data set have balanced data and as a reason I have considered accuracy as a metric by dividing the number of samples that were properly categorised by the total sample count. It shows how often the model generated accurate predictions.\n",
    "\n",
    "To fit the Thoracic Surgery data in this code, we used a variety of models, including logistic regression, SVM polynomial kernel, and decision tree models for both random and grid search. We then evaluated their performance using the metric accuracy. High accuracy score shows that the model represents the proportion of correct predictions made. In comparison of overall scores decision tree model for both random and grid search model outperformed the logistic regression and svm for random, and grid search models when we look into the accuracy scores of all other models, with a better accuracy score of 86.01%, even though both logistic and svm random search are also with 86.01% as mentioned previously decision tree has scored 86.01% score for both the searches and the accuracy values of logistic regression and svm grid search are standing at 84.80% and 80.54%,  showing that the decision tree for both random and grid search model fits best when compared to all the models for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b056086e24cb5602cbcb82122035cd3d6ee2ccbf5df29c16e348c108b0f83be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
